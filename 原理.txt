朴素贝叶斯法是典型的生成学习方法。 
生成方法由训练数据学习联合概率分布P(X,Y),
然后求得后验概率分布P(Y|X)。
具体来说,利用训练数据学习P(XY)和P(Y)的估计,
得到联合概率分布:P(X,Y)=P(Y)P(X|Y)概率估计方法可以是极大似然估计或贝叶斯估计。

线性支持向量机：
SVM基本模型是定义在特征空间上的二分类线性分类器（可推广为多分类），
学习策略为间隔最大化，可形式化为一个求解凸二次规划问题，也等价于正则化的合页损失函数的最小化问题。
求解算法为序列最小最优化算法（SMO）

决策树的原理：根据树结构进行决策，可以用于分类和回归。
一颗决策树包括一个根结点、若干个内部节点和若干个叶节点。
从根节点出发，对每个特征划分数据集并计算信息增益（或者增益率，基尼系数），选择信息增益最大的特征作为划分特征，依次递归，
直至特征划分时信息增益很小或无特征可划分，形成决策树。
****************************************************************************************************
决策树
优点	1. 计算复杂度不高; 2. 输出结果易于理解; 3. 不需要数据预处理; 4. 对中间值的缺失不敏感; 5. 可以处理不相关特征数据; 6. 对于异常点的容错率高
缺点	1. 可能产生过拟合的现象; 2. 对于比较复杂的关系很难学习; 3. 样本发生一点点变化会导致树的结构剧烈变动

***************************************************************************************************

逻辑回归原理：
和二分类情况类似的， 我们仍然可以假设。不同的是这里y不再是0或者1，而是一个向量，例如样本隶属于第二分类，则。我们仍然对模型做基于GML上的假设，，即每个类别i都有自己的参数：
仔细观察，你会发现这是一个softmax函数！该模型的对数似然函数为
这样，我们依然可以用梯度下降或者随机梯度下降解决该问题。
**********************************************************************************************************************
我们使用sklearn.linear_model中的LogisticRegression方法来训练逻辑回归分类器，其主要参数如下：

class_weight：用于处理类别不平衡问题，即这时的阈值不再是0.5，而是一个再缩放后的值；

fit_intercept：bool型参数，设置是否求解截距项，即b，默认True；

random_state：设置随机数种子；

solver：选择用于求解最大化“对数似然”的算法，有以下几种及其适用场景：

　　1.对于较小的数据集，使用"liblinear"更佳；

　　2.对于较大的数据集，"sag"、"saga"更佳；

　　3.对于多分类问题，应使用"newton-cg"、"sag"、"saga"、"lbfgs"；

max_iter：设置求解算法的迭代次数，仅适用于solver设置为"newton-cg"、"lbfgs"、"sag"的情况； 

multi_class：为多分类问题选择训练策略，有"ovr"、"multinomial" ，后者不支持"liblinear"；

n_jobs：当处理多分类问题训练策略为'ovr'时，在训练时并行运算使用的CPU核心数量。当solver被设置为“liblinear”时，不管是否指定了multi_class,这个参数都会被忽略。如果给定值-1，则所有的核心都被使用，所以推荐-1，默认项为1，即只使用1个核心。
********************************************************************************************************************
K-最近邻居：
如果一个样本在特征空间中的k个最邻近的样本中的大多数属于某一个类别，则该样本也划分为这个类别。 KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

该算法假定所有的实例对应于N维欧式空间Ân中的点。通过计算一个点与其他所有点之间的距离，取出与该点最近的K个点，然后统计这K个点里面所属分类比例最大的，则这个点属于该分类。


mlp神经网络：
多层感知机的层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。
求解最佳的参数是一个最优化问题，解决最优化问题，最简单的就是梯度下降法了（sgd）：首先随机初始化所有参数，然后迭代地训练，不断地计算梯度和更新参数，直到满足某个条件为止（比如误差足够小、迭代次数足够多时）。这个过程涉及到代价函数、规则化（Regularization）、学习速率（learning rate）、梯度计算等。
****************************************************************************************************************************************************************************************************************
参数说明: 
1. hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。 

2. activation :激活函数,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认relu 

- identity：f(x) = x 

- logistic：其实就是sigmod,f(x) = 1 / (1 + exp(-x)). 

- tanh：f(x) = tanh(x). 

- relu：f(x) = max(0, x) 

3. solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重 

- lbfgs：quasi-Newton方法的优化器 

- sgd：随机梯度下降 

- adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 

注意：默认solver ‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。 

4. alpha :float,可选的，默认0.0001,正则化项参数 

5. batch_size : int , 可选的，默认’auto’,随机优化的minibatches的大小batch_size=min(200,n_samples)，如果solver是’lbfgs’，分类器将不使用minibatch 

6. learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant 

- ‘constant’: 有’learning_rate_init’给定的恒定学习率 

- ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t) 

- ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. 

7. power_t: double, 可选, default 0.5，只有solver=’sgd’时使用，是逆扩展学习率的指数.当learning_rate=’invscaling’，用来更新有效学习率。 

8. max_iter: int，可选，默认200，最大迭代次数。 

9. random_state:int 或RandomState，可选，默认None，随机数生成器的状态或种子。 

10. shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。 

11. tol：float, 可选，默认1e-4，优化的容忍度 

12. learning_rate_int:double,可选，默认0.001，初始学习率，控制更新权重的补偿，只有当solver=’sgd’ 或’adam’时使用。 

14. verbose : bool, 可选, 默认False,是否将过程打印到stdout 

15. warm_start : bool, 可选, 默认False,当设置成True，使用之前的解决方法作为初始拟合，否则释放之前的解决方法。 

16. momentum : float, 默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用. 

17. nesterovs_momentum : boolean, 默认True, Whether to use Nesterov’s momentum. 只有solver=’sgd’并且momentum > 0使用. 

18. early_stopping : bool, 默认False,只有solver=’sgd’或者’adam’时有效,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续迭代改善，低于tol时终止训练。 

19. validation_fraction : float, 可选, 默认 0.1,用作早期停止验证的预留训练数据集的比例，早0-1之间，只当early_stopping=True有用 

20. beta_1 : float, 可选, 默认0.9，只有solver=’adam’时使用，估计一阶矩向量的指数衰减速率，[0,1)之间 

21. beta_2 : float, 可选, 默认0.999,只有solver=’adam’时使用估计二阶矩向量的指数衰减速率[0,1)之间 

22. epsilon : float, 可选, 默认1e-8,只有solver=’adam’时使用数值稳定值。

**********************************************************************************************************************************
bagging：
首先随机取出一个样本放入采样集合中，再把这个样本放回初始数据集，重复K次采样，最终我们可以获得一个大小为K的样本集合。同样的方法， 我们可以采样出T个含K个样本的采样集合，然后基于每个采样集合训练出一个基学习器，再将这些基学习器进行结合。